import org.apache.spark.streaming.{Seconds, StreamingContext}
import org.apache.spark.ml.feature.{RegexTokenizer, StopWordsRemover}
import org.apache.spark.sql.functions._

object TextStreamCleaner {
  def main(args: Array[String]): Unit = {
    // Use existing SparkContext from REPL
    val ssc = new StreamingContext(sc, Seconds(5))
    val sparkSession = spark  // Use existing SparkSession from REPL

    import sparkSession.implicits._

    val lines = ssc.socketTextStream("localhost", 9999)

    lines.foreachRDD { rdd =>
      if (!rdd.isEmpty()) {
        val df = rdd.toDF("text")

        // Tokenization
        val tokenizer = new RegexTokenizer()
          .setInputCol("text")
          .setOutputCol("words")
          .setPattern("\\W")

        val tokenizedDF = tokenizer.transform(df)

        // Stop words removal
        val remover = new StopWordsRemover()
          .setInputCol("words")
          .setOutputCol("filtered")

        val cleanedDF = remover.transform(tokenizedDF)

        cleanedDF.select("filtered").show(false)
      }
    }

    ssc.start()
    ssc.awaitTermination()
  }
}
